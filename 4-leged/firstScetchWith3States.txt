#include <Servo.h>
#include <NewPing.h>
#define trigPin 10
#define echoPin 6
#define maxDistance 300
#define servo11Pin 11
#define servo12Pin 10
#define servo21Pin 9
#define servo22Pin 6


NewPing ultrasonic(trigPin, echoPin, maxDistance);
Servo servo11;        //upper left joint
Servo servo12;        //upper right joint
Servo servo21;        //lower left joint
Servo servo22;        //lower right joint

float gamma = 0.75;      //look-ahead weight;                 those two parametrs need to be rechecked
float alpha = 0.1;       //"Forgetfulness" weight.  The closer this is to 1 the more weight is given to recent samples.

//Parameters for getAction()
float epsilon;           //epsilon is the probability of choosing an action randomly.  1-epsilon is the probability of choosing the optimal action

//parametrs that are equal to servo 1, 2, 3 and 4 
const int numOfStates = 3;           // number of states that can be done by servos  
const float maxValue = 135.0;              //max and min Values shows maximum and minimum angle for servos
const float minValue = 45.0;
const float delta = (maxValue - minValue)/(float(numOfStates)-1.0);    //The change in servo1's angle when an action is performed on it
const float delayTime = 2.5 * delta;                                   //The time in ms for the servo to move delta

//servo11 parametrs
float initialAngle11 = 90.0;           //current angle of servo11
int state11 = int((initialAngle11 - minValue)/delta);                  //This is an integer between zero and (numOfStates - 1) used to index the state number of servo11   

//servo12 parametrs
float initialAngle12 = 90.0;           //current angle of servo12
int state12 = int((initialAngle12 - minValue)/delta);                  //This is an integer between zero and (numOfStates - 1) used to index the state number of servo12   

//servo21 parametrs
float initialAngle21 = 90.0;           //current angle of servo21
int state21 = int((initialAngle21 - minValue)/delta);                  //This is an integer between zero and (numOfStates - 1) used to index the state number of servo21

//servo22 parametrs
float initialAngle22 = 90.0;           //current angle of servo22
int state22 = int((initialAngle22 - minValue)/delta);                  //This is an integer between zero and (numOfStates - 1) used to index the state number of servo22

//Initialize Q to zeros
const int states = numOfStates * numOfStates * numOfStates * numOfStates; //we have 4 servos and each servo has 3 states, so according combination multiplicity law there are 3^4 states
const int actions = 8;              //for each servo there are two actions(add 45 degree or sub 45), so there are 2 * 4 actions
float Q[states][actions];           //Q matrix(contains the likelyness of each action on different state

//Initialize the state number. The state number is calculated using the state11, state12, state21 and state22 number, multiplying it by 1, 3, 9 and 27
//to get individual state number with each positionThis is the row index of the state in the matrix Q. Starts indexing at 40.
int state = int(state11 + (state12*3) + (state21*9) + (state22*27));
int sPrime;

//Initialize vars for getDeltaDistanceWalked()
float distanceNew = 0.0;
float distanceOld = 0.0;
float deltaDistance = 0.0;

//These get used in the main loop
float r = 0.0;
float lookAheadValue = 0.0;
float sample = 0.0;
int a = 0;

void setup(){
  servo11.attach(servo11Pin);
  servo12.attach(servo12Pin);
  servo21.attach(servo21Pin);
  servo22.attach(servo22Pin);
  servo11.write(initialAngle11);
  servo12.write(initialAngle12);
  servo21.write(initialAngle21);
  servo22.write(initialAngle22);
  Serial.begin(9600);
  delay(5000); 
}

//Returns an action from 0 to 8
int getAction(){
  int action;  //action to be returned
  float valMax = -10000000.0; //value that is the most likely to be done
  float val;   //value to be compered with max val
  int aMax;    //index of the most likely action
  float randVal; //value to be randomized
  int allowedActions[actions] = {-1, -1, -1, -1, -1, -1, -1, -1};  //-1 if action of the index takes you outside the state space.  +1 otherwise
  bool randomActionFound = false;
  
  //find the optimal action.  Exclude actions that take you outside the allowed-state space. for servo11
  if((state11 + 1) != numOfStates){
    allowedActions[0] = 1;
    val = Q[state][0];
    if(val > valMax){
      valMax = val;
      aMax = 0;
    }
  }
  if(state11 != 0){
    allowedActions[1] = 1;
    val = Q[state][1];
    if(val > valMax){
      valMax = val;
      aMax = 1;
    }
  }
  //for servo12
  if((state12 + 1) != numOfStates){
    allowedActions[2] = 1;
    val = Q[state][2];
    if(val > valMax){
      valMax = val;
      aMax = 2;
    }
  }
  if(state12 != 0){
    allowedActions[3] = 1;
    val = Q[state][3];
    if(val > valMax){
      valMax = val;
      aMax = 3;
    }
  }
  //for servo21
  if((state21 + 1) != numOfStates){
    allowedActions[4] = 1;
    val = Q[state][4];
    if(val > valMax){
      valMax = val;
      aMax = 4;
    }
  }
  if(state21 != 0){
    allowedActions[5] = 1;
    val = Q[state][5];
    if(val > valMax){
      valMax = val;
      aMax = 5;
    }
  }
  //for servo22
  if((state22 + 1) != numOfStates){
    allowedActions[6] = 1;
    val = Q[state][6];
    if(val > valMax){
      valMax = val;
      aMax = 6;
    }
  }
  if(state22 != 0){
    allowedActions[7] = 1;
    val = Q[state][7];
    if(val > valMax){
      valMax = val;
      aMax = 7;
    }
  }
  //implement epsilon greedy
  randVal = float(random(0,101));
  if(randVal < (1.0-epsilon)*100.0){    //choose the optimal action with probability 1-epsilon
    action = aMax;
  }else{
    while(!randomActionFound){
      action = int(random(0,8));        //otherwise pick an action between 0 and 8 randomly (inclusive), but don't use actions that take you outside the state-space
      if(allowedActions[action] == 1){
        randomActionFound = true;
      }
    }
  }    
  return(action);
}

//Given action and the global(s) find the next state.  Also keep track of the individual indexes
void setSPrime(int action){  
  if (action == 0)
    state11++;
  else if (action == 1)
    state11--;
  else if (action == 2)
    state12++;
  else if (action == 3)
    state12--;
  else if (action == 4)
    state21++;
  else if (action == 5)
    state21--;
  else if (action == 6)
    state22++;
  else
    state22--; 
  sPrime = int(state11 + (state12*3) + (state21*9) + (state22*27));
}

//Update the position of the servos (this is the physical state transition command)
void setPhysicalState(int action){
  float currentAngle;
  float finalAngle;
  int mSec;
  if (action == 0){
    currentAngle = servo11.read();
    finalAngle = currentAngle + delta;
    servo11.write(finalAngle);
  }else if (action == 1){
    currentAngle = servo11.read();
    finalAngle = currentAngle - delta;
    servo11.write(finalAngle);
  }
  else if (action == 2){
    currentAngle = servo12.read();
    finalAngle = currentAngle + delta;
    servo12.write(finalAngle);
  }else if (action == 3){
    currentAngle = servo12.read();
    finalAngle = currentAngle - delta;
    servo12.write(finalAngle);
  }
  else if (action == 4){
    currentAngle = servo21.read();
    finalAngle = currentAngle + delta;
    servo21.write(finalAngle);
  }else if (action == 5){
    currentAngle = servo21.read();
    finalAngle = currentAngle - delta;
    servo21.write(finalAngle);
  }
  else if (action == 6){
    currentAngle = servo22.read();
    finalAngle = currentAngle + delta;
    servo22.write(finalAngle);
  }else{
    currentAngle = servo22.read();
    finalAngle = currentAngle - delta;
    servo22.write(finalAngle);
  }
  mSec = millis();
  while((millis() - mSec) < delayTime){}
}

//Get the reward using the distance the agent has moved since the last call
float getDeltaDistanceWalked(){
  //get current distance
  distanceNew = float(ultrasonic.ping());   
  deltaDistance = distanceNew - distanceOld;
  if (abs(deltaDistance) < 57.0 || abs(deltaDistance) > 230.0)        //don't count noise
    deltaDistance = 0.0;
  distanceOld = distanceNew;
  return deltaDistance;
}

//Get max over a' of Q(s',a'), but be careful not to look at actions which take the agent outside of the allowed state space
float getLookAhead(){
  float valMax = -100000.0;
  float val;
  //for servo11
  if((state11 + 1) != numOfStates){
    val = Q[sPrime][0];
    if(val > valMax)
      valMax = val;
  }
  if(state11 != 0){
    val = Q[sPrime][1];
    if(val > valMax)
      valMax = val;
  }
  //for servo12
  if((state12 + 1) != numOfStates){
    val = Q[sPrime][2];
    if(val > valMax)
      valMax = val;
  }
  if(state12 != 0){
    val = Q[sPrime][3];
    if(val > valMax)
      valMax = val;
  }
  //for servo21
  if((state21 + 1) != numOfStates){
    val = Q[sPrime][4];
    if(val > valMax)
      valMax = val;
  }
  if(state21 != 0){
    val = Q[sPrime][5];
    if(val > valMax)
      valMax = val;
  }
  //for servo22
  if((state22 + 1) != numOfStates){
    val = Q[sPrime][6];
    if(val > valMax)
      valMax = val;
  }
  if(state22 != 0){
    val = Q[sPrime][7];
    if(val > valMax)
      valMax = val;
  }
  return valMax;
}

void printQ(){
  for(int i = 0; i < states; i++){
    for(int j = 0; j < actions; j++){
      Serial.print(Q[i][j]);
      Serial.print(" ");
    }
    Serial.println(" ");
  }
  Serial.println(" ");
}

void initializeQ(){
  for(int i = 0; i < states; i++){
    for(int j = 0; j < actions; j++){
      Q[i][j] = 10.0;               //Initialize to a positive number to represent optimism over all state-actions
    }
  }
}

int mSec = 0;
const int walkDelay = 70;                   //allow time for the agent to walk after it sets its physical state
const float explorationMinutes = 1.0;        //the desired exploration time in minutes 
const float explorationConst = (explorationMinutes * 60.0)/((float(walkDelay))/1000.0);  //this is the approximate exploration time in units of number of times through the loop

int t = 0;
void loop(){
  t++;
  epsilon = exp(-float(t)/explorationConst);
  a = getAction();           //a is beween 0 and 8
  setSPrime(a);              
  setPhysicalState(a);
  mSec = millis();
  while((millis() - mSec) < walkDelay){}                      //put a delay after the physical action occurs so the agent has time to move before measuring the new position (before calling getDeltaDistanceRolled)
  r = getDeltaDistanceWalked();
  lookAheadValue = getLookAhead();
  sample = r + gamma*lookAheadValue;
  Q[state][a] = Q[state][a] + alpha*(sample - Q[state][a]);
  state = sPrime;
  
  if(t == 2){                //need to reset Q at the beginning since a spurious value arises at the first initialization (something from the rangefinder..)
    initializeQ();
  }
  
  if(t == int(explorationConst)){
    printQ();
  }
}